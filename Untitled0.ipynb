{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Collaborative filtering recommendation system"
      ],
      "metadata": {
        "id": "ZxEmegwZySO6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oAAR5a0SwDZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "30aa0557-91b7-40ff-aed2-9476fa7b0ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   userId  movieId\n",
            "0       1     2011\n",
            "1       1     4144\n",
            "2       1     5767\n",
            "3       1     6711\n",
            "4       1     7318\n",
            "userId     0\n",
            "movieId    0\n",
            "dtype: int64\n",
            "Number of rows in test data: 5000019\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the test CSV file\n",
        "test_data_path = '/content/test.csv'\n",
        "test_data_df = pd.read_csv(test_data_path)\n",
        "\n",
        "# Display the first few rows of the test data\n",
        "print(test_data_df.head())\n",
        "\n",
        "# Check for missing data\n",
        "print(test_data_df.isnull().sum())\n",
        "\n",
        "# Check the number of rows in the test data\n",
        "num_rows_test_data = len(test_data_df)\n",
        "print(f\"Number of rows in test data: {num_rows_test_data}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training CSV file\n",
        "train_data_path = '/content/train.csv'\n",
        "train_data_df = pd.read_csv(train_data_path)\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(train_data_df.head())\n",
        "\n",
        "# Check for missing data\n",
        "print(train_data_df.isnull().sum())\n",
        "\n",
        "# Check the number of rows in the training data\n",
        "num_rows_train_data = len(train_data_df)\n",
        "print(f\"Number of rows in training data: {num_rows_train_data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "91Ov43ZTydvR",
        "outputId": "24df0b63-1058-4787-9083-375e1c826223"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   userId  movieId  rating   timestamp\n",
            "0    5163    57669     4.0  1518349992\n",
            "1  106343        5     4.5  1206238739\n",
            "2  146790     5459     5.0  1076215539\n",
            "3  106362    32296     2.0  1423042565\n",
            "4    9041      366     3.0   833375837\n",
            "userId       0\n",
            "movieId      0\n",
            "rating       0\n",
            "timestamp    0\n",
            "dtype: int64\n",
            "Number of rows in training data: 10000038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values in movieId, rating, or timestamp columns\n",
        "train_data_df.dropna(subset=['movieId', 'rating', 'timestamp'], inplace=True)\n",
        "\n",
        "# Verify that there are no missing values left\n",
        "print(train_data_df.isnull().sum())\n",
        "print(f\"Number of rows in training data after dropping missing values: {len(train_data_df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GJHcdIIny1lj",
        "outputId": "37a5b39e-72bf-4101-87f4-d65241054d43"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "userId       0\n",
            "movieId      0\n",
            "rating       0\n",
            "timestamp    0\n",
            "dtype: int64\n",
            "Number of rows in training data after dropping missing values: 10000038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import cross_validate, train_test_split\n",
        "\n",
        "# Load the train and test CSV files\n",
        "train_data_path = '/content/train.csv'\n",
        "test_data_path = '/content/test.csv'\n",
        "\n",
        "train_data_df = pd.read_csv(train_data_path)\n",
        "test_data_df = pd.read_csv(test_data_path)\n",
        "\n",
        "# Drop rows with missing values in movieId, rating, or timestamp columns in train_data_df\n",
        "train_data_df.dropna(subset=['movieId', 'rating', 'timestamp'], inplace=True)\n",
        "\n",
        "# Convert the training data into a format suitable for collaborative filtering\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(train_data_df[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# Sample a subset of the training data\n",
        "train_data_subset = train_data_df.sample(frac=0.4, random_state=42)  # Adjust frac as needed\n",
        "data_subset = Dataset.load_from_df(train_data_subset[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# Build and train the model (using SVD as an example)\n",
        "algo = SVD()\n",
        "\n",
        "# Perform cross-validation on the subset to evaluate RMSE\n",
        "cross_validate_results = cross_validate(algo, data_subset, measures=['RMSE'], cv=5, verbose=True)\n",
        "\n",
        "# Print the RMSE scores from cross-validation\n",
        "print(f\"RMSE scores: {cross_validate_results['test_rmse']}\")\n",
        "\n",
        "# Convert the subset into Surprise Dataset\n",
        "data_subset = Dataset.load_from_df(train_data_subset[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# Convert the subset into Surprise Dataset\n",
        "data_subset = Dataset.load_from_df(train_data_subset[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# Split the subset into train and test sets (optional)\n",
        "train_subset, test_subset = train_test_split(data_subset, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the algorithm on the subset\n",
        "algo.fit(train_subset)\n",
        "\n",
        "# Generate predictions for the test data using batch processing\n",
        "user_ids = test_data_df['userId'].values\n",
        "movie_ids = test_data_df['movieId'].values\n",
        "\n",
        "batch_size = 10000  # Adjust batch size based on system's capabilities\n",
        "predictions = []\n",
        "\n",
        "for start_idx in range(0, len(user_ids), batch_size):\n",
        "    end_idx = start_idx + batch_size\n",
        "    user_batch = user_ids[start_idx:end_idx]\n",
        "    movie_batch = movie_ids[start_idx:end_idx]\n",
        "\n",
        "    batch_predictions = [algo.predict(user_id, movie_id).est for user_id, movie_id in zip(user_batch, movie_batch)]\n",
        "    predictions.extend(batch_predictions)\n",
        "\n",
        "# Create the submission DataFrame\n",
        "submission_df = pd.DataFrame({\n",
        "    'Id': test_data_df['userId'].astype(str) + '_' + test_data_df['movieId'].astype(str),\n",
        "    'rating': predictions\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_path = '/content/submission.csv'\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "\n",
        "print(f\"Submission file saved to {submission_path}\")\n",
        "\n",
        "# Verify the number of rows in the submission file\n",
        "num_rows_submission = len(submission_df)\n",
        "print(f\"Number of rows in submission file: {num_rows_submission}\")\n",
        "\n",
        "# Check the header\n",
        "submission_header = list(submission_df.columns)\n",
        "expected_header = ['Id', 'rating']\n",
        "print(f\"Submission header: {submission_header}\")\n",
        "print(f\"Expected header: {expected_header}\")\n",
        "\n",
        "# Verify the number of rows\n",
        "expected_rows = 5000019\n",
        "if num_rows_submission == expected_rows:\n",
        "    print(\"The submission file meets the requirements.\")\n",
        "else:\n",
        "    print(\"The submission file does not meet the requirements.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF0BK4ZWzFD9",
        "outputId": "272d9ddc-f3e8-4b9b-8f61-4426b3762e0e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating RMSE of algorithm SVD on 5 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
            "RMSE (testset)    0.8667  0.8667  0.8668  0.8673  0.8666  0.8668  0.0003  \n",
            "Fit time          167.13  170.22  170.77  180.35  174.04  172.50  4.50    \n",
            "Test time         31.40   30.86   32.49   30.00   26.03   30.16   2.22    \n",
            "RMSE scores: [0.86666628 0.86668463 0.86678662 0.86734576 0.86656945]\n",
            "Submission file saved to /content/submission.csv\n",
            "Number of rows in submission file: 5000019\n",
            "Submission header: ['Id', 'rating']\n",
            "Expected header: ['Id', 'rating']\n",
            "The submission file meets the requirements.\n"
          ]
        }
      ]
    }
  ]
}